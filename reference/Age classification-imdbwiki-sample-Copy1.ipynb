{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b191786e",
   "metadata": {},
   "source": [
    "# Age Classification Using Facial Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0339d9",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d0d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\virtualenv\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import plotly.express as px \n",
    "import pandas as pd \n",
    "import cv2 \n",
    "import os \n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Activation, Dropout, Flatten, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#added import for sgd\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# import dlib\n",
    "import dlib\n",
    "\n",
    "import dlib\n",
    "\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "\n",
    "import imutils\n",
    "\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290bb78",
   "metadata": {},
   "source": [
    "## Data Retrival and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc1b92",
   "metadata": {},
   "source": [
    "#### Converting the txt data to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3fdc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "      <th>AgeRange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/99/nm0625099_rm316640512_1974-11-2_2...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/90/nm0004790_rm365789952_1976-8-23_2...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/68/nm0356468_rm3801258496_1975-8-4_1...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/58/nm1864458_rm1663605504_1975-9-17_...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>male</td>\n",
       "      <td>wiki_crop/56/29835356_1989-07-25_2014.jpg</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211096</th>\n",
       "      <td>48</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/81/nm0000381_rm1343402752_1962-7-19_...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211097</th>\n",
       "      <td>42</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/62/nm1101562_rm723357184_1959-10-7_2...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211098</th>\n",
       "      <td>29</td>\n",
       "      <td>female</td>\n",
       "      <td>wiki_crop/22/3571322_1977-01-31_2007.jpg</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211099</th>\n",
       "      <td>57</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/16/nm0000616_rm2154482176_1956-4-18_...</td>\n",
       "      <td>Old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211100</th>\n",
       "      <td>15</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/89/nm0094789_rm991411456_1967-5-31_1...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211101 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  gender                                               path  \\\n",
       "0        30    male  imdb_crop/99/nm0625099_rm316640512_1974-11-2_2...   \n",
       "1        27    male  imdb_crop/90/nm0004790_rm365789952_1976-8-23_2...   \n",
       "2        23    male  imdb_crop/68/nm0356468_rm3801258496_1975-8-4_1...   \n",
       "3        29    male  imdb_crop/58/nm1864458_rm1663605504_1975-9-17_...   \n",
       "4        24    male          wiki_crop/56/29835356_1989-07-25_2014.jpg   \n",
       "...     ...     ...                                                ...   \n",
       "211096   48    male  imdb_crop/81/nm0000381_rm1343402752_1962-7-19_...   \n",
       "211097   42    male  imdb_crop/62/nm1101562_rm723357184_1959-10-7_2...   \n",
       "211098   29  female           wiki_crop/22/3571322_1977-01-31_2007.jpg   \n",
       "211099   57    male  imdb_crop/16/nm0000616_rm2154482176_1956-4-18_...   \n",
       "211100   15  female  imdb_crop/89/nm0094789_rm991411456_1967-5-31_1...   \n",
       "\n",
       "       AgeRange  \n",
       "0         Youth  \n",
       "1         Youth  \n",
       "2         Youth  \n",
       "3         Youth  \n",
       "4         Youth  \n",
       "...         ...  \n",
       "211096    Adult  \n",
       "211097    Adult  \n",
       "211098    Youth  \n",
       "211099      Old  \n",
       "211100    Youth  \n",
       "\n",
       "[211101 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_IMDB_WIKI_non_gray.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0569d13",
   "metadata": {},
   "source": [
    "#### Dropping null values and resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7147b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc85dc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211101, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fc3b7e",
   "metadata": {},
   "source": [
    "#### Using the details of dataframe to get the image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae4f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_path'] = '/age and gender prediction/project/data/'+ df['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d8128b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Youth    104604\n",
       "Adult     83935\n",
       "Old       18921\n",
       "Kid        3641\n",
       "Name: AgeRange, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['AgeRange'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d031905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youth = df[df['AgeRange'] == 'Youth']\n",
    "df_youth = df_youth.sample(frac = 1)\n",
    "#df_youth_test1 = df_youth.tail(200)\n",
    "df_youth = df_youth.head(2000)\n",
    "df_youth_test = df_youth.head(500)\n",
    "#df_youth_test = pd.concat([df_youth_test, df_youth_test1])\n",
    "\n",
    "df_adult = df[df['AgeRange'] == 'Adult']\n",
    "df_adult = df_adult.sample(frac = 1)\n",
    "#df_adult_test1 = df_adult.tail(200)\n",
    "df_adult = df_adult.head(2000)\n",
    "df_adult_test = df_adult.head(500)\n",
    "#df_adult_test = pd.concat([df_adult_test, df_adult_test1])\n",
    "\n",
    "df_kid = df[df['AgeRange'] == 'Kid']\n",
    "df_kid = df_kid.sample(frac = 1)\n",
    "#df_kid_test1 = df_kid.tail(200)\n",
    "df_kid = df_kid.head(2000)\n",
    "df_kid_test = df_kid.head(500)\n",
    "#df_kid_test = pd.concat([df_kid_test, df_kid_test1])\n",
    "\n",
    "df_old = df[df['AgeRange'] == 'Old']\n",
    "df_old = df_old.sample(frac = 1)\n",
    "#df_old_test1 = df_old.tail(200)\n",
    "df_old = df_old.head(2000)\n",
    "df_old_test = df_old.head(500)\n",
    "#df_old_test = pd.concat([df_old_test, df_old_test1])\n",
    "\n",
    "df_train = pd.concat([df_youth, df_adult, df_kid, df_old])\n",
    "df_test = pd.concat([df_youth_test, df_adult_test, df_kid_test, df_old_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a0504c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = df[['AgeRange', 'age', 'gender', 'image_path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d25b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d004603",
   "metadata": {},
   "source": [
    "#### Using LabelEncoder to obtain targets in integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a50db367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "le = LabelEncoder()\n",
    "df_train['AgeRange'] = le.fit_transform(df_train['AgeRange'])\n",
    "df_test['AgeRange'] = le.fit_transform(df_test['AgeRange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de4fa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "      <th>AgeRange</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101156</th>\n",
       "      <td>34</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/35/nm0004735_rm3460017920_1977-3-8_2...</td>\n",
       "      <td>3</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69181</th>\n",
       "      <td>22</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/77/nm0144377_rm403806720_1978-5-11_2...</td>\n",
       "      <td>3</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185985</th>\n",
       "      <td>27</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/13/nm1897713_rm3462772992_1984-3-25_...</td>\n",
       "      <td>3</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117380</th>\n",
       "      <td>23</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/89/nm2645189_rm1986768896_1987-1-2_2...</td>\n",
       "      <td>3</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121767</th>\n",
       "      <td>34</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/24/nm0688624_rm768578560_1973-11-15_...</td>\n",
       "      <td>3</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75167</th>\n",
       "      <td>63</td>\n",
       "      <td>female</td>\n",
       "      <td>wiki_crop/95/154695_1944-03-26_2008.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>/age and gender prediction/project/data/wiki_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197634</th>\n",
       "      <td>75</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/28/nm0780528_rm3727862528_1913-10-28...</td>\n",
       "      <td>2</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159187</th>\n",
       "      <td>61</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/39/nm0122439_rm570461952_1948-1-14_2...</td>\n",
       "      <td>2</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207918</th>\n",
       "      <td>61</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/47/nm0001347_rm2755954432_1948-2-5_2...</td>\n",
       "      <td>2</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31251</th>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>wiki_crop/95/27243995_1941-09-25_2009.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>/age and gender prediction/project/data/wiki_c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  gender                                               path  \\\n",
       "101156   34    male  imdb_crop/35/nm0004735_rm3460017920_1977-3-8_2...   \n",
       "69181    22  female  imdb_crop/77/nm0144377_rm403806720_1978-5-11_2...   \n",
       "185985   27  female  imdb_crop/13/nm1897713_rm3462772992_1984-3-25_...   \n",
       "117380   23  female  imdb_crop/89/nm2645189_rm1986768896_1987-1-2_2...   \n",
       "121767   34  female  imdb_crop/24/nm0688624_rm768578560_1973-11-15_...   \n",
       "...     ...     ...                                                ...   \n",
       "75167    63  female            wiki_crop/95/154695_1944-03-26_2008.jpg   \n",
       "197634   75    male  imdb_crop/28/nm0780528_rm3727862528_1913-10-28...   \n",
       "159187   61    male  imdb_crop/39/nm0122439_rm570461952_1948-1-14_2...   \n",
       "207918   61  female  imdb_crop/47/nm0001347_rm2755954432_1948-2-5_2...   \n",
       "31251    67    male          wiki_crop/95/27243995_1941-09-25_2009.jpg   \n",
       "\n",
       "        AgeRange                                         image_path  \n",
       "101156         3  /age and gender prediction/project/data/imdb_c...  \n",
       "69181          3  /age and gender prediction/project/data/imdb_c...  \n",
       "185985         3  /age and gender prediction/project/data/imdb_c...  \n",
       "117380         3  /age and gender prediction/project/data/imdb_c...  \n",
       "121767         3  /age and gender prediction/project/data/imdb_c...  \n",
       "...          ...                                                ...  \n",
       "75167          2  /age and gender prediction/project/data/wiki_c...  \n",
       "197634         2  /age and gender prediction/project/data/imdb_c...  \n",
       "159187         2  /age and gender prediction/project/data/imdb_c...  \n",
       "207918         2  /age and gender prediction/project/data/imdb_c...  \n",
       "31251          2  /age and gender prediction/project/data/wiki_c...  \n",
       "\n",
       "[8000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fbaa8f",
   "metadata": {},
   "source": [
    "#### Dumping the same for future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03b6ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdbwiki_age_encoder_8000.pkl','wb') as pkl_file:\n",
    "    pickle.dump(le, pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a5e726",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ca3fa",
   "metadata": {},
   "source": [
    "#### Preparing to split for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d858d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[['image_path']].values \n",
    "y_train = df_train[['AgeRange']].values \n",
    "\n",
    "X_test = df_test[['image_path']].values \n",
    "y_test = df_test[['AgeRange']].values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fdc7274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd31d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15281658",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1149349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split \n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670cb39e",
   "metadata": {},
   "source": [
    "#### Assigning uniform image extensions and resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed4b4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(individual_path):\n",
    "    img = tf.io.read_file(np.array(individual_path).ravel()[0]) \n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    img = tf.image.resize(img, [227,227])\n",
    "    return img "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7cbc0",
   "metadata": {},
   "source": [
    "## Face Detection and Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbec76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_training_values(X_train,y_train):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "    fa = FaceAligner(predictor, desiredFaceWidth=256)\n",
    "    for image_path, value in zip(X_train, y_train):\n",
    "        imageP = image_path[0].decode(\"utf-8\")\n",
    "        img= cv2.imread(imageP, 1)\n",
    "        denoised_image = cv2.fastNlMeansDenoisingColored(img, None, 5, 6, 7, 21)\n",
    "\n",
    "        gray = cv2.cvtColor(denoised_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Detect the face\n",
    "        rects = detector(gray, 1)\n",
    "        # Detect landmarks for each face\n",
    "        \n",
    "        try:\n",
    "            for rect in rects:\n",
    "                faceAligned = fa.align(img, gray, rect)\n",
    "\n",
    "            gray1 = cv2.cvtColor(faceAligned, cv2.COLOR_BGR2GRAY)\n",
    "            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            faces = face_cascade.detectMultiScale(gray1, 1.3, 5)\n",
    "\n",
    "            try:\n",
    "                for (x,y,w,h) in faces:\n",
    "                    # for putting rectangle on face\n",
    "                    #cv2.rectangle(faceAligned, (x,y), (x+w, y+h), (0, 255, 0),3)\n",
    "                    roi_color = faceAligned[y:y+h, x:x+w]\n",
    "                    cv2.imwrite(imageP , roi_color)\n",
    "            except:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        image = preprocess_image([bytes(imageP, 'utf-8')])\n",
    "        yield image, value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a38c2",
   "metadata": {},
   "source": [
    "#### Using train and test for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0166cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_generator(yield_training_values,\n",
    "                                          args=[X_train, y_train],\n",
    "                                          output_types=(tf.float32, tf.float32),\n",
    "                                          output_shapes=([227, 227, 3], [1]))\n",
    "\n",
    "\n",
    "ds_test = tf.data.Dataset.from_generator(yield_training_values,\n",
    "                                          args=[X_test, y_test],\n",
    "                                          output_types=(tf.float32, tf.float32),\n",
    "                                          output_shapes=([227, 227, 3], [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c162a0",
   "metadata": {},
   "source": [
    "### Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "702b6b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "ds_train = ds_train.cache().shuffle(buffer_size=1000).batch(32).prefetch(buffer_size=AUTOTUNE)\n",
    "ds_test = ds_test.cache().shuffle(buffer_size=1000).batch(32).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb232d",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f1553",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f582d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "  tf.keras.layers.RandomZoom(0.2,0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fd760",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02d2a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 8s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\virtualenv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (227, 227, 3)\n",
    "\n",
    "# Create a frozen base model\n",
    "base_model = tf.keras.applications.InceptionV3(include_top=False, pooling = None)\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[-5:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Create input and output layers\n",
    "inputs = tf.keras.layers.Input(shape=input_shape, name=\"input_layer\") # create input layer\n",
    "x = base_model(inputs) # pass augmented \n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "x = tf.keras.layers.Dense(512)(x)\n",
    "x = tf.keras.layers.Dense(256)(x)\n",
    "outputs = tf.keras.layers.Dense(4, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.001), # use Adam optimizer with base learning rate\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58618626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 227, 227, 3)]     0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, None, None, 2048)  21802784  \n",
      "                                                                 \n",
      " global_average_pooling_laye  (None, 2048)             0         \n",
      " r (GlobalAveragePooling2D)                                      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,984,228\n",
      "Trainable params: 1,181,444\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe48162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# head = keras.models.Sequential([\n",
    "#     keras.layers.Conv2D(filters=96, kernel_size=(3,3), strides=(1,1), activation='relu'),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "#     keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "#     keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.MaxPool2D(pool_size=(3,3), strides=((2,2))),\n",
    "#     keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.MaxPool2D(pool_size=(3,3), strides=((2,2))),\n",
    "#     keras.layers.Flatten(),\n",
    "#     keras.layers.Dense(512, activation='relu'),\n",
    "#     keras.layers.Dropout(0.5),\n",
    "#     keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(l=0.01)),\n",
    "#     keras.layers.Dropout(0.5),\n",
    "#     keras.layers.Dense(4, activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cce17f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential([\n",
    "#     data_augmentation,\n",
    "#     base_model,\n",
    "#     keras.layers.Conv2D(filters=96, kernel_size=(3,3), strides=(1,1), activation='relu'),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "#     keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "#     keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.MaxPool2D(pool_size=(3,3), strides=((2,2))),\n",
    "#     keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "#     keras.layers.BatchNormalization(),\n",
    "#     keras.layers.MaxPool2D(pool_size=(3,3), strides=((2,2))),\n",
    "#     keras.layers.Flatten(),\n",
    "#     keras.layers.Dense(512, activation='relu'),\n",
    "#     keras.layers.Dropout(0.5),\n",
    "#     keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(l=0.01)),\n",
    "#     keras.layers.Dropout(0.5),\n",
    "#     keras.layers.Dense(4, activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72976e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 227, 227, 3)]     0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, None, None, 2048)  21802784  \n",
      "                                                                 \n",
      " global_average_pooling_laye  (None, 2048)             0         \n",
      " r (GlobalAveragePooling2D)                                      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,984,228\n",
      "Trainable params: 1,181,444\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b993d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"model_checkpoints_weights/imdbwiki/age_checkpoint_14mar.ckpt\"\n",
    "\n",
    "# Create a ModelCheckpoint callback that saves the model's weights only\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=False, # set to False to save the entire model\n",
    "                                                         save_best_only=True, # set to True to save only the best model instead of a model every epoch \n",
    "                                                         save_freq=\"epoch\", # save every epoch\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ac08e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam = tf.keras.optimizers.Adam(learning_rate=0.001) \n",
    "# #sgd = SGD(learning_rate=0.001)\n",
    "# model.compile(optimizer=adam, loss = tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ab85a",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a19e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    250/Unknown - 5353s 19s/step - loss: 49.4244 - accuracy: 0.6784\n",
      "Epoch 1: val_loss improved from inf to 11.43447, saving model to model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 6266s 22s/step - loss: 49.4244 - accuracy: 0.6784 - val_loss: 11.4345 - val_accuracy: 0.3390\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - ETA: 0s - loss: 3.5253 - accuracy: 0.7045\n",
      "Epoch 2: val_loss improved from 11.43447 to 6.95259, saving model to model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1447s 6s/step - loss: 3.5253 - accuracy: 0.7045 - val_loss: 6.9526 - val_accuracy: 0.2570\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.9078 - accuracy: 0.6981\n",
      "Epoch 3: val_loss improved from 6.95259 to 5.84452, saving model to model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1675s 7s/step - loss: 1.9078 - accuracy: 0.6981 - val_loss: 5.8445 - val_accuracy: 0.2590\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.8670 - accuracy: 0.7035\n",
      "Epoch 4: val_loss improved from 5.84452 to 3.08923, saving model to model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1836s 7s/step - loss: 1.8670 - accuracy: 0.7035 - val_loss: 3.0892 - val_accuracy: 0.2745\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.2243 - accuracy: 0.7146\n",
      "Epoch 5: val_loss did not improve from 3.08923\n",
      "250/250 [==============================] - 1770s 7s/step - loss: 1.2243 - accuracy: 0.7146 - val_loss: 4.6985 - val_accuracy: 0.2660\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.7692 - accuracy: 0.6990\n",
      "Epoch 6: val_loss did not improve from 3.08923\n",
      "250/250 [==============================] - 1769s 7s/step - loss: 2.7692 - accuracy: 0.6990 - val_loss: 3.3352 - val_accuracy: 0.3055\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.3157 - accuracy: 0.7244\n",
      "Epoch 7: val_loss did not improve from 3.08923\n",
      "250/250 [==============================] - 1749s 7s/step - loss: 1.3157 - accuracy: 0.7244 - val_loss: 4.0735 - val_accuracy: 0.2555\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.5526 - accuracy: 0.7101\n",
      "Epoch 8: val_loss improved from 3.08923 to 2.92347, saving model to model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints_weights/imdbwiki\\age_checkpoint_14mar.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1773s 7s/step - loss: 1.5526 - accuracy: 0.7101 - val_loss: 2.9235 - val_accuracy: 0.3425\n",
      "Epoch 9/10\n",
      "242/250 [============================>.] - ETA: 53s - loss: 3.8282 - accuracy: 0.6769"
     ]
    }
   ],
   "source": [
    "history = model.fit(ds_train, validation_data=ds_test, epochs=10, callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation and training data separately\n",
    "def plot_loss_curves(history):\n",
    "    \"\"\"\n",
    "    Returns separate loss curves for training and validation metrics.\n",
    "    \"\"\" \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(history.history['loss']))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(epochs, loss, label='training_loss')\n",
    "    plt.plot(epochs, val_loss, label='val_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/imdbwiki/age_14mar_final.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47333ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007659d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d911989",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.load_model(r'E:\\age and gender prediction\\project\\models\\imdbwiki\\age_14mar.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49291769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1246fae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
