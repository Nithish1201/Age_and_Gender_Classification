{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956d4d5f",
   "metadata": {},
   "source": [
    "# Gender Classification using Facial Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93af11",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d0d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\virtualenv\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import plotly.express as px \n",
    "import pandas as pd \n",
    "import cv2 \n",
    "import os \n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Activation, Dropout, Flatten, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#added import for sgd\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# #for Landmark detection\n",
    "# import dlib\n",
    "import dlib\n",
    "\n",
    "import dlib\n",
    "\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "\n",
    "import imutils\n",
    "\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69396332",
   "metadata": {},
   "source": [
    "## Date Retrival and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8388ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "      <th>AgeRange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>male</td>\n",
       "      <td>wiki_crop/09/43981209_1990-07-17_2015.jpg</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/08/nm0651008_rm1017367040_1970-10-15...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/01/nm0000701_rm1272548096_1975-10-5_...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/35/nm0001435_rm233299200_1963-7-30_1...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/29/nm0005129_rm2932918528_1976-4-20_...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2106</th>\n",
       "      <td>17</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/07/nm0430107_rm3292896000_1987-2-9_2...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>38</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/17/nm0000417_rm3158022912_1964-4-20_...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>12</td>\n",
       "      <td>female</td>\n",
       "      <td>wiki_crop/30/24972730_2000-04-09_2013.jpg</td>\n",
       "      <td>Kid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>56</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/47/nm0000547_rm3455170816_1953-5-24_...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>63</td>\n",
       "      <td>male</td>\n",
       "      <td>wiki_crop/53/479153_1948-08-20_2012.jpg</td>\n",
       "      <td>Old</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2111 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  gender                                               path AgeRange\n",
       "0      24    male          wiki_crop/09/43981209_1990-07-17_2015.jpg    Youth\n",
       "1      41    male  imdb_crop/08/nm0651008_rm1017367040_1970-10-15...    Adult\n",
       "2      33  female  imdb_crop/01/nm0000701_rm1272548096_1975-10-5_...    Youth\n",
       "3      30  female  imdb_crop/35/nm0001435_rm233299200_1963-7-30_1...    Youth\n",
       "4      33    male  imdb_crop/29/nm0005129_rm2932918528_1976-4-20_...    Youth\n",
       "...   ...     ...                                                ...      ...\n",
       "2106   17    male  imdb_crop/07/nm0430107_rm3292896000_1987-2-9_2...    Youth\n",
       "2107   38    male  imdb_crop/17/nm0000417_rm3158022912_1964-4-20_...    Adult\n",
       "2108   12  female          wiki_crop/30/24972730_2000-04-09_2013.jpg      Kid\n",
       "2109   56    male  imdb_crop/47/nm0000547_rm3455170816_1953-5-24_...    Adult\n",
       "2110   63    male            wiki_crop/53/479153_1948-08-20_2012.jpg      Old\n",
       "\n",
       "[2111 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sample_IMDB_WIKI_non_gray.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e961f3",
   "metadata": {},
   "source": [
    "#### Dropping null values and resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b2bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af19e331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2111, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa3daf",
   "metadata": {},
   "source": [
    "#### Using the details of dataframe to get the image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae4f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_path'] = '/age and gender prediction/project/data/'+ df['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e0ab48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "      <th>AgeRange</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>20</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/56/nm1409956_rm1012765184_1986-1-29_...</td>\n",
       "      <td>Youth</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>53</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/35/nm0866835_rm3490183936_1917-5-7_1...</td>\n",
       "      <td>Adult</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>44</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/32/nm0000332_rm3246296832_1964-11-29...</td>\n",
       "      <td>Adult</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>35</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/16/nm0358316_rm224965632_1971-3-10_2...</td>\n",
       "      <td>Adult</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>25</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/33/nm2079733_rm539592960_1983-12-2_2...</td>\n",
       "      <td>Youth</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>wiki_crop/42/32809942_1990-05-09_2013.jpg</td>\n",
       "      <td>Youth</td>\n",
       "      <td>/age and gender prediction/project/data/wiki_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>28</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/57/nm0005057_rm3466959616_1970-10-6_...</td>\n",
       "      <td>Youth</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>45</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/05/nm0121605_rm2110238208_1966-11-25...</td>\n",
       "      <td>Adult</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>47</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/61/nm0000161_rm3077108224_1966-9-2_2...</td>\n",
       "      <td>Adult</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>44</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/73/nm0000973_rm587568896_1963-12-16_...</td>\n",
       "      <td>Adult</td>\n",
       "      <td>/age and gender prediction/project/data/imdb_c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2111 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  gender                                               path AgeRange  \\\n",
       "1325   20    male  imdb_crop/56/nm1409956_rm1012765184_1986-1-29_...    Youth   \n",
       "1542   53    male  imdb_crop/35/nm0866835_rm3490183936_1917-5-7_1...    Adult   \n",
       "945    44    male  imdb_crop/32/nm0000332_rm3246296832_1964-11-29...    Adult   \n",
       "994    35    male  imdb_crop/16/nm0358316_rm224965632_1971-3-10_2...    Adult   \n",
       "764    25  female  imdb_crop/33/nm2079733_rm539592960_1983-12-2_2...    Youth   \n",
       "...   ...     ...                                                ...      ...   \n",
       "626    22    male          wiki_crop/42/32809942_1990-05-09_2013.jpg    Youth   \n",
       "130    28  female  imdb_crop/57/nm0005057_rm3466959616_1970-10-6_...    Youth   \n",
       "1445   45    male  imdb_crop/05/nm0121605_rm2110238208_1966-11-25...    Adult   \n",
       "281    47  female  imdb_crop/61/nm0000161_rm3077108224_1966-9-2_2...    Adult   \n",
       "706    44    male  imdb_crop/73/nm0000973_rm587568896_1963-12-16_...    Adult   \n",
       "\n",
       "                                             image_path  \n",
       "1325  /age and gender prediction/project/data/imdb_c...  \n",
       "1542  /age and gender prediction/project/data/imdb_c...  \n",
       "945   /age and gender prediction/project/data/imdb_c...  \n",
       "994   /age and gender prediction/project/data/imdb_c...  \n",
       "764   /age and gender prediction/project/data/imdb_c...  \n",
       "...                                                 ...  \n",
       "626   /age and gender prediction/project/data/wiki_c...  \n",
       "130   /age and gender prediction/project/data/imdb_c...  \n",
       "1445  /age and gender prediction/project/data/imdb_c...  \n",
       "281   /age and gender prediction/project/data/imdb_c...  \n",
       "706   /age and gender prediction/project/data/imdb_c...  \n",
       "\n",
       "[2111 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1991283",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b684a49",
   "metadata": {},
   "source": [
    "#### Mapping target to float value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703e42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['gender'] = new_df['gender'].apply(lambda x : 1 if x == 'male' else 0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18584dbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    1201\n",
       "0.0     910\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e025f",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fb2ad",
   "metadata": {},
   "source": [
    "#### Preparing to split for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d858d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df[['image_path']].values \n",
    "y = new_df[['gender']].values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58fb8f",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af6ccc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0aa37",
   "metadata": {},
   "source": [
    "#### Assigning uniform image extensions and resizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed4b4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(individual_path):\n",
    "    img = tf.io.read_file(np.array(individual_path).ravel()[0]) \n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    img = tf.image.resize(img, [227,227])\n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "817caa24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['/age and gender prediction/project/data/wiki_crop/66/6902966_1970-08-19_2011.jpg'],\n",
       "       ['/age and gender prediction/project/data/wiki_crop/76/98676_1936-10-16_1990.jpg'],\n",
       "       ['/age and gender prediction/project/data/imdb_crop/58/nm0001458_rm3537619200_1926-4-30_1975.jpg'],\n",
       "       ...,\n",
       "       ['/age and gender prediction/project/data/imdb_crop/68/nm1310368_rm1903199232_1980-6-24_2010.jpg'],\n",
       "       ['/age and gender prediction/project/data/imdb_crop/25/nm3232025_rm2186722560_1986-2-15_2009.jpg'],\n",
       "       ['/age and gender prediction/project/data/imdb_crop/74/nm3158974_rm160494336_1988-6-12_2015.jpg']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "299fbedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/age and gender prediction/project/data/imdb_crop/06/nm0308606_rm125104128_1964-2-20_2014.jpg'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9453cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread(X_train[100][0], 1)\n",
    "# cv2.imshow('img', img)\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49112c62",
   "metadata": {},
   "source": [
    "## Face Detection, Face Alignment, Landmark Detection, and Image Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbec76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_training_values(X_train,y_train):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "    fa = FaceAligner(predictor, desiredFaceWidth=227)\n",
    "    for image_path, value in zip(X_train, y_train):\n",
    "        imageP = image_path[0].decode(\"utf-8\")\n",
    "        img= cv2.imread(imageP, 1)\n",
    "        denoised_image = cv2.fastNlMeansDenoisingColored(img, None, 5, 6, 7, 21)\n",
    "        \n",
    "        gray = cv2.cvtColor(denoised_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Detect the face\n",
    "        rects = detector(gray, 1)\n",
    "        # Detect landmarks for each face\n",
    "        try:\n",
    "            for rect in rects:\n",
    "                faceAligned = fa.align(img, gray, rect)\n",
    "\n",
    "            gray1 = cv2.cvtColor(faceAligned, cv2.COLOR_BGR2GRAY)\n",
    "            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            faces = face_cascade.detectMultiScale(gray1, 1.3, 5)\n",
    "\n",
    "            try:\n",
    "                for (x,y,w,h) in faces:\n",
    "                    # to put rectangle on face\n",
    "                    #cv2.rectangle(faceAligned, (x,y), (x+w, y+h), (0, 255, 0),3)\n",
    "                    roi_color = faceAligned[y:y+h, x:x+w]\n",
    "                    cv2.imwrite(imageP , roi_color)\n",
    "            except:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        image = preprocess_image([bytes(imageP, 'utf-8')])\n",
    "        yield image, value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109d38b",
   "metadata": {},
   "source": [
    "#### Using train and test for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0166cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_generator(yield_training_values,\n",
    "                                          args=[X_train, y_train],\n",
    "                                          output_types=(tf.float32, tf.float32),\n",
    "                                          output_shapes=([227, 227, 3], [1]))\n",
    "\n",
    "\n",
    "ds_test = tf.data.Dataset.from_generator(yield_training_values,\n",
    "                                          args=[X_test, y_test],\n",
    "                                          output_types=(tf.float32, tf.float32),\n",
    "                                          output_shapes=([227, 227, 3], [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c4826b",
   "metadata": {},
   "source": [
    "### Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "702b6b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "ds_train = ds_train.cache().shuffle(buffer_size=1000).batch(32).prefetch(buffer_size=AUTOTUNE)\n",
    "ds_test = ds_test.cache().shuffle(buffer_size=1000).batch(32).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d874d",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e1462",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f582d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "  tf.keras.layers.RandomZoom(0.2,0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7b321",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cce17f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    data_augmentation,\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(7,7), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=((2,2))),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=((2,2))),\n",
    "    keras.layers.Flatten(),\n",
    "    #dense change from 4096\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    #dense change from 4096\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    #activation change from softmax\n",
    "    #dense change from 2\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73ccdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"model_checkpoints_weights/imdbwiki/gender_sample_checkpoint_9mar.ckpt\" # note: remember saving directly to Colab is temporary\n",
    "\n",
    "# Create a ModelCheckpoint callback that saves the model's weights only\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True, # set to False to save the entire model\n",
    "                                                         save_best_only=True, # set to True to save only the best model instead of a model every epoch \n",
    "                                                         save_freq=\"epoch\", # save every epoch\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ac08e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 80\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.9\n",
    "sgd = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "#loss and optimiser change from sparse and adam\n",
    "model.compile(optimizer=sgd, loss = tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy', Precision() , Recall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166b0c2",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f9a19e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "     53/Unknown - 986s 8s/step - loss: 1.4814 - accuracy: 0.5101 - precision: 0.5678 - recall: 0.5838\n",
      "Epoch 1: val_loss improved from inf to 18.34063, saving model to model_checkpoints_weights/imdbwiki\\gender_sample_checkpoint_9mar.ckpt\n",
      "53/53 [==============================] - 1264s 13s/step - loss: 1.4814 - accuracy: 0.5101 - precision: 0.5678 - recall: 0.5838 - val_loss: 18.3406 - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.8878 - accuracy: 0.5468 - precision: 0.5994 - recall: 0.6150\n",
      "Epoch 2: val_loss improved from 18.34063 to 0.71126, saving model to model_checkpoints_weights/imdbwiki\\gender_sample_checkpoint_9mar.ckpt\n",
      "53/53 [==============================] - 65s 1s/step - loss: 0.8878 - accuracy: 0.5468 - precision: 0.5994 - recall: 0.6150 - val_loss: 0.7113 - val_accuracy: 0.5674 - val_precision: 0.5680 - val_recall: 0.9917\n",
      "Epoch 3/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.7721 - accuracy: 0.5332 - precision: 0.5797 - recall: 0.6545\n",
      "Epoch 3: val_loss did not improve from 0.71126\n",
      "53/53 [==============================] - 66s 1s/step - loss: 0.7721 - accuracy: 0.5332 - precision: 0.5797 - recall: 0.6545 - val_loss: 0.7846 - val_accuracy: 0.4468 - val_precision: 0.6500 - val_recall: 0.0542\n",
      "Epoch 4/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - ETA: 0s - loss: 0.7188 - accuracy: 0.5361 - precision: 0.5795 - recall: 0.6753\n",
      "Epoch 4: val_loss improved from 0.71126 to 0.68265, saving model to model_checkpoints_weights/imdbwiki\\gender_sample_checkpoint_9mar.ckpt\n",
      "53/53 [==============================] - 67s 1s/step - loss: 0.7188 - accuracy: 0.5361 - precision: 0.5795 - recall: 0.6753 - val_loss: 0.6826 - val_accuracy: 0.5745 - val_precision: 0.6364 - val_recall: 0.5833\n",
      "Epoch 5/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6964 - accuracy: 0.5533 - precision: 0.5776 - recall: 0.8012\n",
      "Epoch 5: val_loss improved from 0.68265 to 0.67533, saving model to model_checkpoints_weights/imdbwiki\\gender_sample_checkpoint_9mar.ckpt\n",
      "53/53 [==============================] - 68s 1s/step - loss: 0.6964 - accuracy: 0.5533 - precision: 0.5776 - recall: 0.8012 - val_loss: 0.6753 - val_accuracy: 0.5957 - val_precision: 0.6507 - val_recall: 0.6208\n",
      "Epoch 6/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.5450 - precision: 0.5796 - recall: 0.7315\n",
      "Epoch 6: val_loss did not improve from 0.67533\n",
      "53/53 [==============================] - 68s 1s/step - loss: 0.6912 - accuracy: 0.5450 - precision: 0.5796 - recall: 0.7315 - val_loss: 0.6813 - val_accuracy: 0.5390 - val_precision: 0.6257 - val_recall: 0.4667\n",
      "Epoch 7/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6980 - accuracy: 0.5575 - precision: 0.5808 - recall: 0.8002\n",
      "Epoch 7: val_loss improved from 0.67533 to 0.67030, saving model to model_checkpoints_weights/imdbwiki\\gender_sample_checkpoint_9mar.ckpt\n",
      "53/53 [==============================] - 68s 1s/step - loss: 0.6980 - accuracy: 0.5575 - precision: 0.5808 - recall: 0.8002 - val_loss: 0.6703 - val_accuracy: 0.5626 - val_precision: 0.5733 - val_recall: 0.8958\n",
      "Epoch 8/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6906 - accuracy: 0.5610 - precision: 0.5795 - recall: 0.8345\n",
      "Epoch 8: val_loss did not improve from 0.67030\n",
      "53/53 [==============================] - 67s 1s/step - loss: 0.6906 - accuracy: 0.5610 - precision: 0.5795 - recall: 0.8345 - val_loss: 0.6715 - val_accuracy: 0.5697 - val_precision: 0.5690 - val_recall: 0.9958\n",
      "Epoch 9/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6817 - accuracy: 0.5770 - precision: 0.5979 - recall: 0.7846\n",
      "Epoch 9: val_loss did not improve from 0.67030\n",
      "53/53 [==============================] - 67s 1s/step - loss: 0.6817 - accuracy: 0.5770 - precision: 0.5979 - recall: 0.7846 - val_loss: 0.6736 - val_accuracy: 0.5697 - val_precision: 0.5707 - val_recall: 0.9750\n",
      "Epoch 10/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6838 - accuracy: 0.5735 - precision: 0.5914 - recall: 0.8117\n",
      "Epoch 10: val_loss did not improve from 0.67030\n",
      "53/53 [==============================] - 66s 1s/step - loss: 0.6838 - accuracy: 0.5735 - precision: 0.5914 - recall: 0.8117 - val_loss: 0.6729 - val_accuracy: 0.5650 - val_precision: 0.5718 - val_recall: 0.9292\n",
      "Epoch 11/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.5586 - precision: 0.5807 - recall: 0.8085\n",
      "Epoch 11: val_loss did not improve from 0.67030\n",
      "53/53 [==============================] - 66s 1s/step - loss: 0.6888 - accuracy: 0.5586 - precision: 0.5807 - recall: 0.8085 - val_loss: 0.6771 - val_accuracy: 0.5674 - val_precision: 0.5872 - val_recall: 0.8000\n",
      "Epoch 12/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6759 - accuracy: 0.5717 - precision: 0.5931 - recall: 0.7888\n",
      "Epoch 12: val_loss improved from 0.67030 to 0.66623, saving model to model_checkpoints_weights/imdbwiki\\gender_sample_checkpoint_9mar.ckpt\n",
      "53/53 [==============================] - 67s 1s/step - loss: 0.6759 - accuracy: 0.5717 - precision: 0.5931 - recall: 0.7888 - val_loss: 0.6662 - val_accuracy: 0.5887 - val_precision: 0.6078 - val_recall: 0.7750\n",
      "Epoch 13/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.5592 - precision: 0.5894 - recall: 0.7440\n",
      "Epoch 13: val_loss did not improve from 0.66623\n",
      "53/53 [==============================] - 62s 1s/step - loss: 0.6862 - accuracy: 0.5592 - precision: 0.5894 - recall: 0.7440 - val_loss: 0.6718 - val_accuracy: 0.5626 - val_precision: 0.5914 - val_recall: 0.7417\n",
      "Epoch 14/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6809 - accuracy: 0.5735 - precision: 0.5928 - recall: 0.8012\n",
      "Epoch 14: val_loss did not improve from 0.66623\n",
      "53/53 [==============================] - 62s 1s/step - loss: 0.6809 - accuracy: 0.5735 - precision: 0.5928 - recall: 0.8012 - val_loss: 0.6713 - val_accuracy: 0.5721 - val_precision: 0.5974 - val_recall: 0.7542\n",
      "Epoch 15/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6766 - accuracy: 0.5829 - precision: 0.5979 - recall: 0.8169\n",
      "Epoch 15: val_loss did not improve from 0.66623\n",
      "53/53 [==============================] - 62s 1s/step - loss: 0.6766 - accuracy: 0.5829 - precision: 0.5979 - recall: 0.8169 - val_loss: 0.6718 - val_accuracy: 0.5768 - val_precision: 0.6063 - val_recall: 0.7250\n",
      "Epoch 16/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6746 - accuracy: 0.5729 - precision: 0.5896 - recall: 0.8221\n",
      "Epoch 16: val_loss did not improve from 0.66623\n",
      "53/53 [==============================] - 63s 1s/step - loss: 0.6746 - accuracy: 0.5729 - precision: 0.5896 - recall: 0.8221 - val_loss: 0.6742 - val_accuracy: 0.5768 - val_precision: 0.5764 - val_recall: 0.9583\n",
      "Epoch 17/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6735 - accuracy: 0.5717 - precision: 0.5979 - recall: 0.7565\n",
      "Epoch 17: val_loss did not improve from 0.66623\n",
      "53/53 [==============================] - 66s 1s/step - loss: 0.6735 - accuracy: 0.5717 - precision: 0.5979 - recall: 0.7565 - val_loss: 0.6777 - val_accuracy: 0.5650 - val_precision: 0.5673 - val_recall: 0.9833\n",
      "Epoch 18/80\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6728 - accuracy: 0.6013 - precision: 0.6121 - recall: 0.8179\n",
      "Epoch 18: val_loss did not improve from 0.66623\n",
      "53/53 [==============================] - 62s 1s/step - loss: 0.6728 - accuracy: 0.6013 - precision: 0.6121 - recall: 0.8179 - val_loss: 0.6779 - val_accuracy: 0.5579 - val_precision: 0.5810 - val_recall: 0.7917\n",
      "Epoch 19/80\n",
      "26/53 [=============>................] - ETA: 30s - loss: 0.6755 - accuracy: 0.5841 - precision: 0.5997 - recall: 0.8168"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6448/2008676699.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\virtualenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\virtualenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\virtualenv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\virtualenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\virtualenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\virtualenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\virtualenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32me:\\virtualenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\virtualenv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(ds_train, validation_data=ds_test, epochs=epochs, callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation and training data separately\n",
    "def plot_loss_curves(history):\n",
    "    \"\"\"\n",
    "    Returns separate loss curves for training and validation metrics.\n",
    "    \"\"\" \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(history.history['loss']))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(epochs, loss, label='training_loss')\n",
    "    plt.plot(epochs, val_loss, label='val_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5184a88",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d330f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/imdbwiki/gender_mar9_sample.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c56138",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf68204",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad573aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0da68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
