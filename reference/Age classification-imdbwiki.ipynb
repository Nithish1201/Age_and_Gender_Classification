{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b191786e",
   "metadata": {},
   "source": [
    "# Age Classification Using Facial Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0339d9",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d0d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\virtualenv\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import plotly.express as px \n",
    "import pandas as pd \n",
    "import cv2 \n",
    "import os \n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Activation, Dropout, Flatten, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#added import for sgd\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# import dlib\n",
    "import dlib\n",
    "\n",
    "import dlib\n",
    "\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "\n",
    "import imutils\n",
    "\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290bb78",
   "metadata": {},
   "source": [
    "## Data Retrival and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc1b92",
   "metadata": {},
   "source": [
    "#### Converting the txt data to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3fdc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "      <th>AgeRange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>male</td>\n",
       "      <td>wiki_crop/09/43981209_1990-07-17_2015.jpg</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/08/nm0651008_rm1017367040_1970-10-15...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/01/nm0000701_rm1272548096_1975-10-5_...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>female</td>\n",
       "      <td>imdb_crop/35/nm0001435_rm233299200_1963-7-30_1...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/29/nm0005129_rm2932918528_1976-4-20_...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2106</th>\n",
       "      <td>17</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/07/nm0430107_rm3292896000_1987-2-9_2...</td>\n",
       "      <td>Youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>38</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/17/nm0000417_rm3158022912_1964-4-20_...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>12</td>\n",
       "      <td>female</td>\n",
       "      <td>wiki_crop/30/24972730_2000-04-09_2013.jpg</td>\n",
       "      <td>Kid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>56</td>\n",
       "      <td>male</td>\n",
       "      <td>imdb_crop/47/nm0000547_rm3455170816_1953-5-24_...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>63</td>\n",
       "      <td>male</td>\n",
       "      <td>wiki_crop/53/479153_1948-08-20_2012.jpg</td>\n",
       "      <td>Old</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2111 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  gender                                               path AgeRange\n",
       "0      24    male          wiki_crop/09/43981209_1990-07-17_2015.jpg    Youth\n",
       "1      41    male  imdb_crop/08/nm0651008_rm1017367040_1970-10-15...    Adult\n",
       "2      33  female  imdb_crop/01/nm0000701_rm1272548096_1975-10-5_...    Youth\n",
       "3      30  female  imdb_crop/35/nm0001435_rm233299200_1963-7-30_1...    Youth\n",
       "4      33    male  imdb_crop/29/nm0005129_rm2932918528_1976-4-20_...    Youth\n",
       "...   ...     ...                                                ...      ...\n",
       "2106   17    male  imdb_crop/07/nm0430107_rm3292896000_1987-2-9_2...    Youth\n",
       "2107   38    male  imdb_crop/17/nm0000417_rm3158022912_1964-4-20_...    Adult\n",
       "2108   12  female          wiki_crop/30/24972730_2000-04-09_2013.jpg      Kid\n",
       "2109   56    male  imdb_crop/47/nm0000547_rm3455170816_1953-5-24_...    Adult\n",
       "2110   63    male            wiki_crop/53/479153_1948-08-20_2012.jpg      Old\n",
       "\n",
       "[2111 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sample_IMDB_WIKI_non_gray.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89521972",
   "metadata": {},
   "source": [
    "#### Dropping null values and resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e61fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc85dc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2111, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fc3b7e",
   "metadata": {},
   "source": [
    "#### Using the details of dataframe to get the image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae4f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_path'] = '/age and gender prediction/project/data/'+ df['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d8128b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Youth    1075\n",
       "Adult     808\n",
       "Old       186\n",
       "Kid        42\n",
       "Name: AgeRange, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['AgeRange'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a0504c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[['AgeRange', 'age', 'gender', 'image_path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d25b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d004603",
   "metadata": {},
   "source": [
    "#### Using LabelEncoder to obtain targets in integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a50db367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "le = LabelEncoder()\n",
    "new_df['AgeRange'] = le.fit_transform(new_df['AgeRange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fbaa8f",
   "metadata": {},
   "source": [
    "#### Dumping the same for future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b6ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdbwiki_age_encoder.pkl','wb') as pkl_file:\n",
    "    pickle.dump(le, pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a5e726",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ca3fa",
   "metadata": {},
   "source": [
    "#### Preparing to split for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d858d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df[['image_path']].values \n",
    "y = new_df[['AgeRange']].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fdc7274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y.flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15281658",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1149349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670cb39e",
   "metadata": {},
   "source": [
    "#### Assigning uniform image extensions and resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed4b4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(individual_path):\n",
    "    img = tf.io.read_file(np.array(individual_path).ravel()[0]) \n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    img = tf.image.resize(img, [227,227])\n",
    "    return img "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7cbc0",
   "metadata": {},
   "source": [
    "## Face Detection and Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbec76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_training_values(X_train,y_train):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "    fa = FaceAligner(predictor, desiredFaceWidth=256)\n",
    "    for image_path, value in zip(X_train, y_train):\n",
    "        imageP = image_path[0].decode(\"utf-8\")\n",
    "        img= cv2.imread(imageP, 1)\n",
    "        denoised_image = cv2.fastNlMeansDenoisingColored(img, None, 5, 6, 7, 21)\n",
    "\n",
    "        gray = cv2.cvtColor(denoised_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Detect the face\n",
    "        rects = detector(gray, 1)\n",
    "        # Detect landmarks for each face\n",
    "        \n",
    "        try:\n",
    "            for rect in rects:\n",
    "                faceAligned = fa.align(img, gray, rect)\n",
    "\n",
    "            gray1 = cv2.cvtColor(faceAligned, cv2.COLOR_BGR2GRAY)\n",
    "            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            faces = face_cascade.detectMultiScale(gray1, 1.3, 5)\n",
    "\n",
    "            try:\n",
    "                for (x,y,w,h) in faces:\n",
    "                    # for putting rectangle on face\n",
    "                    #cv2.rectangle(faceAligned, (x,y), (x+w, y+h), (0, 255, 0),3)\n",
    "                    roi_color = faceAligned[y:y+h, x:x+w]\n",
    "                    cv2.imwrite(imageP , roi_color)\n",
    "            except:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        image = preprocess_image([bytes(imageP, 'utf-8')])\n",
    "        yield image, value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a38c2",
   "metadata": {},
   "source": [
    "#### Using train and test for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0166cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_generator(yield_training_values,\n",
    "                                          args=[X_train, y_train],\n",
    "                                          output_types=(tf.float32, tf.float32),\n",
    "                                          output_shapes=([227, 227, 3], [1]))\n",
    "\n",
    "\n",
    "ds_test = tf.data.Dataset.from_generator(yield_training_values,\n",
    "                                          args=[X_test, y_test],\n",
    "                                          output_types=(tf.float32, tf.float32),\n",
    "                                          output_shapes=([227, 227, 3], [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c162a0",
   "metadata": {},
   "source": [
    "### Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "702b6b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "ds_train = ds_train.cache().shuffle(buffer_size=1000).batch(32).prefetch(buffer_size=AUTOTUNE)\n",
    "ds_test = ds_test.cache().shuffle(buffer_size=1000).batch(32).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb232d",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f1553",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f582d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "  tf.keras.layers.RandomZoom(0.2,0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fd760",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cce17f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    data_augmentation,\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(7,7), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=((2,2))),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=((2,2))),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(l=0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b993d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"model_checkpoints_weights/imdbwiki/age_checkpoint_18jan.ckpt\"\n",
    "\n",
    "# Create a ModelCheckpoint callback that saves the model's weights only\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True, # set to False to save the entire model\n",
    "                                                         save_best_only=True, # set to True to save only the best model instead of a model every epoch \n",
    "                                                         save_freq=\"epoch\", # save every epoch\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ac08e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam = tf.keras.optimizers.Adam(learning_rate=0.001) \n",
    "sgd = SGD(learning_rate=0.001)\n",
    "model.compile(optimizer=sgd, loss = tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ab85a",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a19e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(ds_train, validation_data=ds_test, epochs=20, callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation and training data separately\n",
    "def plot_loss_curves(history):\n",
    "    \"\"\"\n",
    "    Returns separate loss curves for training and validation metrics.\n",
    "    \"\"\" \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(history.history['loss']))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(epochs, loss, label='training_loss')\n",
    "    plt.plot(epochs, val_loss, label='val_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/imdbwiki/age_26nov.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47333ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007659d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d911989",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
